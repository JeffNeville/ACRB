**Author:** Jeffrey Neville  
**Email:** [Jeff@roseandthomas.com](mailto:Jeff@roseandthomas.com)  
**LinkedIn:** [https://www.linkedin.com/in/jefneville/](https://www.linkedin.com/in/jefneville/)  

---

# **Why Most AI Projects Fail (And Why That's Actually Fine)**

**Retail executives are applying the wrong financial metrics to artificial intelligence. A venture-capital mindset offers a better approach.**

A retailer launches seven AI projects expecting six to fail. This is sound strategy, not mismanagement. The portfolio mathematics are straightforward. One 10-fold winner covers six experiments and still delivers net positive returns. Another project that hits 50-fold transforms a business unit. A rare 100-fold return reshapes competitive position. Yet most retail executives still evaluate AI using tools designed for warehouse upgrades and point-of-sale systems. This mismatch limits competitiveness and wastes capital.

The pattern has emerged. AI projects do not behave like traditional IT investments. They follow what venture capitalists call a power law distribution. Most experiments produce modest gains or lose money. A small minority generate very large payoffs. Those few successes generate 80% or more of portfolio returns. Venture funds have lived with this reality for decades. According to Fred Wilson's analysis of Union Square Ventures portfolio patterns, approximately one-third of investments fail, one-third barely break even, and one-third produce most gains. Research from the VC Lab confirms that roughly two-thirds of deals return less than invested capital, while a handful of companies in each fund produce 10-fold, 50-fold or higher returns and carry the entire portfolio.

AI initiatives inside retailers behave similarly. A cluster of pilots in marketing, pricing, demand forecasting and customer service will deliver mixed results. Some fail outright. Some produce small gains. One or two, if use case selection is focused, can transform a business's cost or revenue. Executive teams that cling to deterministic accounting are not avoiding risk. They are concentrating it. By demanding guaranteed returns from each AI initiative, they favor low-variance, low-impact work and quietly rule out the outcomes that change competitive position.

## **Why success metrics mislead**

Recent data mislead. A 2023 IBM Institute for Business Value study surveying 2,500 global executives found average enterprise AI return on investment at 5.9%, below a 10% cost of capital. A 2025 MIT NANDA initiative report titled The GenAI Divide: State of AI in Business 2025, based on 150 interviews with business leaders, a survey of 350 employees, and analysis of 300 public AI deployments, documented that 95% of generative AI pilots delivered zero measurable profit and loss impact. The MIT research defined success narrowly. A project had to move beyond pilot into production and show formal ROI within six months. Anything else counted as failure.

These figures mislead. The MIT work ignored how substantial AI programs mature. Systems require training, workflow integration and staff trust. The process often runs for years. Evaluating such efforts on six-month snapshots constitutes a category error.

Inside many retailers, the result is predictable. AI pilots launch with fanfare, fail to meet strict hurdle rates in early quarters, then shut down. Executives cite pilot purgatory, tighten business-case requirements and repeat the cycle. The 95% failure statistic becomes self-fulfilling. The underlying issue is simple. Organizations apply deterministic tools to probabilistic systems.

Traditional IT projects behave like bonds, delivering modest and reasonably predictable returns. The correct unit of analysis for AI shifts from single project to portfolio. The right questions become: What total return did we obtain from this set of bets taken together? Did we see at least one or two clear breakouts that justify the budget? Did the failures generate useful capabilities, data or skills that make the next round cheaper?

Viewed this way, a pilot that produces no direct revenue lift but creates high-quality datasets, reusable code or a team that understands large language models still has value. Traditional profit-and-loss snapshots miss that entirely.

## **The portfolio pattern emerges**

Portfolio effects appear in recent retail data from retail and adjacent sectors. Finland's S Group, a grocery and department store operator, launched an AI program covering assortment, pricing and operations. One flagship effort applied clustering across more than 1,000 stores to refine product mix. Oliver Wyman research documents improved customer feedback, stronger sales growth and margin expansion after rollout, though the company has not disclosed specific percentage improvements. S Group did not bet on a single application. It pursued sets of use cases and learned which justified enterprise deployment.

Jordan Craig, a mid-size U.S. fashion retailer, tried various AI-driven marketing tactics in 2024\. Email automation proved the breakout hit. According to Klaviyo case study data, using Klaviyo to power predictive customer segments and personalized flows, Jordan Craig saw email revenue jump 54% year over year in six months. Automated campaigns became a top revenue driver, accounting for one-third of total email revenue. Other digital efforts were more modest. A couple of well-chosen AI use cases delivered the majority of growth.

An Indian plywood manufacturer with $360 million in revenue faced a persistent quality problem. Manual inspectors missed defects that triggered customer complaints and hurt margins. In 2024, the company deployed an AI visual inspection system for $1.8 million. Defects fell from 2% to 0.1%, generating $6.87 million in annual savings and a 281% first-year return. That single pilot repaid its cost several times over.

A mid-size consulting firm, in an anonymized case reported by business analytics researchers, found itself testing multiple AI tools across departments with minimal coordination. Marketing was testing three AI copywriters, sales was trying two chatbots, operations had subscribed to an automation tool. The firm spent $8,500 monthly for minimal gain. After establishing a lightweight AI center to triage efforts, they killed redundancies and focused resources. Five pilots succeeded after 12 failed. Total AI spend dropped 48% while usage tripled, yielding an estimated $67,000 in annual savings. The key was treating experiments as a portfolio with explicit kill criteria.

China's major e-commerce platforms provide another view of portfolio dynamics at scale. JD.com launched an enhanced suite of AI-powered solutions in March 2024 designed to cut merchant operational costs by as much as 50%. According to Bain research surveying 500 Chinese small and medium enterprise merchants in August 2024, those actively using generative AI retail tools reported higher satisfaction with their digital operations than non-users. The Alibaba marketplace Taobao deployed Wenwen, an AI shopping assistant, during Singles Day 2023\. By August 2024, Bain consumer surveys found that 12% of Chinese shoppers had used generative AI retail tools in the previous six months, rising to 23% among Generation Z consumers. The platforms ran dozens of concurrent AI experiments. Most produced modest results. A handful drove significant merchant adoption and customer engagement. Alibaba committed $52 billion to AI capital expenditures in 2024, more than the previous ten years combined, reflecting a portfolio strategy where multiple bets fund the search for transformative applications.

Three facts stand out. First, winning use cases often emerge within 12 to 18 months when governance is disciplined. Second, one or two big wins drive the bulk of returns across quality control, efficiency, retail revenue and operations. Third, the 70% of projects that deliver modest results or fail still generate capabilities that reduce the cost of subsequent experiments.

## **When agents reshape commerce**

Since 2024, retailers have confronted a shift in how shopping happens. Agentic commerce describes AI systems that can reason, act, maintain context and coordinate with other tools across the retail value chain. Unlike static chatbots, agentic systems retain memory over time and learn from each interaction. They can orchestrate multiple steps, call APIs, query internal systems and hand off to humans when needed.

The economics are already material. Deloitte's Tech Trends 2025 report surveyed firms on agentic AI adoption. According to this research, 39% of surveyed firms have invested in agentic AI, with 70% seeing positive returns. If conversational agents handling shopping-related prompts convert even a modest share at typical ticket sizes, the implied commerce volume reaches hundreds of billions of dollars annually.

For retailers, agentic commerce introduces two urgent questions. How will customers and their AI agents find and transact with the retailer's assortment when much of the decision process moves into conversational interfaces? Which internal processes can be handled by agents in ways that materially alter cost to serve, speed and quality?

These are portfolio questions. No single agent project will define the future. Instead, there will be a range of trials: shopping concierges, internal buyer agents that adjust bids, operations agents that reorder stock, service agents that resolve support contacts. The power law will apply here as well. A few will generate very large gains. Retailers that treat agentic AI as a curiosity will miss the inflection. Those that treat it as a field of probabilistic bets stand a better chance of discovering the use cases that matter.

### **The economic mechanism behind power law distributions**

AI returns concentrate in a few winners rather than distribute normally because of three reinforcing dynamics. First, successful AI systems generate data flywheels. A recommendation engine that works well attracts more users, produces more interaction data, and becomes more accurate, widening the gap between winners and modest performers. Second, AI projects face high fixed costs in infrastructure, talent and integration but near-zero marginal costs at scale. A system that reaches production can serve millions of additional transactions with minimal incremental investment. Third, organizational learning compounds. Teams that ship one successful AI system build reusable capabilities in data pipelines, model operations and change management that lower the cost and increase the success rate of subsequent projects. These three mechanisms ensure that returns do not spread evenly across a portfolio. Instead, they accumulate in the projects that cross critical adoption thresholds, creating the asymmetric distribution venture capitalists have observed for decades.

## **Portfolio mechanics**

Moving from deterministic to probabilistic thinking requires concrete changes in budgeting, governance and executive behavior. Portfolio thinking starts with separating established use cases from high-uncertainty experiments. Successful retailers typically allocate capital in a 70-30 split. The 70% allocation covers known improvements in forecasting, pricing, labor planning and search relevance. The 30% allocation funds experiments in agentic commerce, new interaction models and more radical process redesign. The critical step is ring-fencing the 30%. It should be treated as research and development with explicit failure tolerance.

At portfolio level, the target is simple. One winner should pay for the entire set of experiments, including failures, with room to spare. A 10-fold return on an individual project cost should cover the full portfolio of five to seven projects. A 50-fold return changes the economics of a function. A 100-fold return reshapes the company.

Evaluation windows should run 12 to 18 months for signs of success, not full profit-and-loss return. Projects should pass through clear stages: concept and feasibility (months one to three), pilot and early indicators (months four to nine), kill or commit (months 10 to 12), and scaling and business impact (months 13 to 18). Projects should rarely drift past 18 months without a firm decision.

Without discipline, tolerance for failure turns into an excuse for endless, low-value experiments. Governance should set kill criteria agreed at the start that define when an experiment must stop. It should also establish clear thresholds for aggressive rollout when pilots exceed expectations.

## **Competitive consequences**

The evidence points to a clear conclusion. Retail chief executives now face a choice. One path is to keep AI within the old IT frame: a string of isolated projects, each justified by a neat spreadsheet and constrained by short time horizons. That approach will yield a smattering of modest gains and confirm the view that AI underdelivers. It will also leave the field open to competitors who treat AI as a portfolio of strategic options and stay the course long enough to discover their 10-fold, 50-fold or 100-fold wins.

The alternative is to accept that serious AI work, especially in the agentic commerce era, will produce different financial patterns. Failure rates will be higher on paper. Financial returns will be lumpy across projects and time. Reporting will have to show portfolio-level outcomes and option value, not only realized profit. For many boards, this will feel uncomfortable. It also looks very similar to the capital allocation decisions they already make when they back international expansion, private labels, new formats or acquisitions. In those contexts, no one expects every store, line or deal to hit its target. The judgment is made on the portfolio. AI deserves the same treatment.

Retailers that restructure governance around portfolio thinking will capture outsize returns. Those who continue applying deterministic metrics to probabilistic technology will fund their competitors' advantages while their own AI budgets deliver predictable mediocrity. The decisive factor is not algorithm selection or technology partnerships. What separates winners from laggards is whether executive teams measure AI the way venture capitalists measure innovation: as a portfolio where concentrated success in a few bets justifies tolerance for frequent failure. Retailers that adopt this approach will discover the 10-fold and 50-fold wins that shift competitive position. Those that demand guaranteed returns from every project will watch rivals capture the asymmetric gains that define the next decade of retail competition.

---

**Sources**

Belcic, I., & Stryker, C. (2023). *How to maximize ROI on AI in 2025\.* IBM Institute for Business Value.

Challapally, A. (2025). *The GenAI Divide: State of AI in Business 2025\.* MIT NANDA Initiative.

Kaput, M. (2025). *That viral MIT study claiming 95% of AI pilots fail? Don't believe the hype.* Marketing AI Institute.

Wilson, F. (2009). *The "1/3, 1/3, 1/3" pattern in VC portfolios.* Union Square Ventures.

VC Lab. (2023). *The power law in VC.* govclab.com.

Andersson, T., & van Wijk, D. (2024). *SOK's AI transformation.* Oliver Wyman.

Thibodeaux, T. (2025). *Deloitte Tech Trends 2025\.* Deloitte Insights.

Klaviyo. (2024). *Case Study: Jordan Craigâ€”54% email revenue lift with AI-driven flows.*

Bain & Company. (2024). *Gen AI can help retailers defy China's economic slowdown.* Bain China Consumer Survey and SME Merchant Survey.

